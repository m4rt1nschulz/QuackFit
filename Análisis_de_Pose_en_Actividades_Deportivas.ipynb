{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "mVbGKtgQ_OkK",
        "dYq3EC6PkvwZ",
        "DkN-MyE-ku7p",
        "R8RVFrEEAu5j"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Análisis de Pose en Actividades Deportivas\n",
        "Se deben instalar las librerías antes de pasar a usar el modelo."
      ],
      "metadata": {
        "id": "Gk_LID-SERXd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importación de Datos e Instalación de Librerías"
      ],
      "metadata": {
        "id": "mVbGKtgQ_OkK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SO_YoiItvm3l",
        "outputId": "e5104c82-ba98-4999-b636-cf88413dcaff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ultralytics in /usr/local/lib/python3.10/dist-packages (8.0.227)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.22.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.23.5)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.8.0.76)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.1)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.31.0)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.11.4)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.1.0+cu121)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.16.0+cu121)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.66.1)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.5.3)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.12.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: thop>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.1.1.post2209072238)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.46.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (23.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2023.3.post1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2023.11.17)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2.1.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "# Instalacion de ultralytics\n",
        "pip install ultralytics\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalacion de timm\n",
        "pip install timm torch torchvision"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPTIPgxW0K75",
        "outputId": "745c9f6d-31e5-4df3-99bc-adc2ed978aa7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting timm\n",
            "  Downloading timm-0.9.12-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.16.0+cu121)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from timm) (0.19.4)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm) (0.4.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (4.66.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (23.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: timm\n",
            "Successfully installed timm-0.9.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Revisar que ultralytics esté funcionando bien\n",
        "import cv2\n",
        "import ultralytics\n",
        "ultralytics.checks()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1t1TpscvwAJ",
        "outputId": "7764602f-64a2-4d89-977f-e3a3d2c6c20d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics YOLOv8.0.227 🚀 Python-3.10.12 torch-2.1.0+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "Setup complete ✅ (2 CPUs, 12.7 GB RAM, 26.2/78.2 GB disk)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importación de datos de Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TIi4dTiY3JdC",
        "outputId": "bd2b2ef3-f823-4502-e24d-be95aa1e820f"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Análisis y Organización del Dataset\n"
      ],
      "metadata": {
        "id": "dYq3EC6PkvwZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Función para convertir videos en frames con poses aisladas en fondo negro\n",
        "from PIL import Image\n",
        "from ultralytics import YOLO\n",
        "import numpy as np\n",
        "\n",
        " # La función recibe 'name' como nombre para el archivo, 'path' como ubicación del archivo y 'end' como la ubicación donde se quiere enviar.\n",
        "def crear_dataset(name, path, end):\n",
        "  model = YOLO('yolov8n-pose.pt') # Modelo YOLO Pose para la detección\n",
        "  results = model(path) # Se aplica detección en el video\n",
        "  n=0\n",
        "  j=0\n",
        "  while j<len(results):\n",
        "    n = n+1\n",
        "    original = results[j].orig_shape\n",
        "    h = int(original[0])\n",
        "    w = int(original[1])\n",
        "    black_img = np.zeros((h,w,3),dtype=np.uint8) # Se genera imagen negra con alto y largo de la imagen original\n",
        "    img_array = results[j].plot(img=black_img, labels=False, boxes=False, probs=False)\n",
        "    im = Image.fromarray(img_array[..., ::-1])\n",
        "    # im.show() en caso de querer ver las imágenes\n",
        "    im_path = end +'/results_'+str(name)+'_'+str(n)+'.jpg'\n",
        "    im.save(im_path)\n",
        "    j = j+1 # Frame a frame (se puede hacer saltando frames si son muchos videos)"
      ],
      "metadata": {
        "id": "UzS8byO1-Jzi"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Función para copiar cierto número de imágenes de una carpeta a otra\n",
        "import os\n",
        "import shutil\n",
        "def copiar_dataset(path,end):\n",
        "  files = os.listdir(path)\n",
        "  for file in files[:200]:\n",
        "    start = os.path.join(path, file)\n",
        "    destiny = os.path.join(end, file)\n",
        "    shutil.copy(start, destiny)"
      ],
      "metadata": {
        "id": "TRTUhvxyDaQu"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separar frames de videos en imágenes\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "def vid_to_frame(video_path, output_path):\n",
        "    # Abre el video\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    # Guardar cada frame del video como imagen\n",
        "    frame_count = 0\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        frame_path = os.path.join(output_path, f\"frame_{frame_count:04d}.png\")\n",
        "        cv2.imwrite(frame_path, frame)\n",
        "        frame_count += 1\n",
        "    # Cierra el video\n",
        "    cap.release()\n",
        "    print(f\"Se extrajeron {frame_count} frames y se guardaron en {output_path}.\")"
      ],
      "metadata": {
        "id": "ePiDMVu98T19"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Entrenamiento"
      ],
      "metadata": {
        "id": "DkN-MyE-ku7p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import timm\n",
        "import torch.optim as optim\n",
        "\n",
        "# Se registra la cantidad de ejercicios distintos, en este caso 5\n",
        "num_classes = 4\n",
        "\n",
        "# Se parte de un modelo preentrenado, en este caso una mobilenet\n",
        "model = timm.create_model('timm/resnet50.a1_in1k', pretrained=True, num_classes=num_classes)\n",
        "\n",
        "# Herramientas para la clasificación\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "vFed1oqt3OyU"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "cuda = torch.cuda.is_available()\n",
        "#Importación datasets\n",
        "train_data_path = '/content/drive/MyDrive/Base de Datos Duckietown/Dataset3/Train'\n",
        "val_data_path = '/content/drive/MyDrive/Base de Datos Duckietown/Dataset3/Test'\n",
        "\n",
        "#Transformaciones a las imágenes (revisar Resize!)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "#ImageFolders para los datasets (pendiente agregar transforms)\n",
        "train_dataset = ImageFolder(root=train_data_path, transform=transform)\n",
        "val_dataset = ImageFolder(root=val_data_path, transform=transform)\n",
        "\n",
        "print(train_dataset.classes)\n",
        "print(val_dataset.classes)\n",
        "\n",
        "#Definir dataset_loaders\n",
        "batch_size = 64\n",
        "kwargs = {'num_workers': 2, 'pin_memory': True} if cuda else {}\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, **kwargs)\n",
        "val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False, **kwargs)\n",
        "\n",
        "#Número de épocas\n",
        "num_epochs = 10\n",
        "\n",
        "if cuda:\n",
        "    model.cuda()\n",
        "#Entrenamiento del modelo con set Train\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    val_total_loss = 0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs = inputs.cuda()\n",
        "        labels = labels.cuda()\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        total_loss += loss.item()\n",
        "        optimizer.step()\n",
        "\n",
        "#Validación del modelo con set Test\n",
        "    with torch.no_grad():\n",
        "        for val_inputs, val_labels in val_loader:\n",
        "            val_inputs = val_inputs.cuda()\n",
        "            val_labels = val_labels.cuda()\n",
        "            val_outputs = model(val_inputs)\n",
        "            val_loss = criterion(val_outputs, val_labels)\n",
        "            val_total_loss += val_loss.item()\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_loader)}, Val Loss: {val_total_loss/len(val_loader)}')\n",
        "\n",
        "#Guardado del modelo\n",
        "model_path = '/content/drive/MyDrive/Base de Datos Duckietown/Modelo4/duckietown-pose-model.pth'\n",
        "torch.save(model.state_dict(), model_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9ad1YQTLQqx",
        "outputId": "600d49f7-f4d3-4566-fc42-5380a79f089b"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['PesoMuerto', 'Press', 'Squat', 'Standing']\n",
            "['PesoMuerto', 'Press', 'Squat', 'Standing']\n",
            "Epoch 1/10, Loss: 0.5153058572775788, Val Loss: 2.010684698820114\n",
            "Epoch 2/10, Loss: 0.0060094143522696365, Val Loss: 2.496789370264326\n",
            "Epoch 3/10, Loss: 0.000926949630310345, Val Loss: 2.61347034573555\n",
            "Epoch 4/10, Loss: 0.0002528099047830673, Val Loss: 2.6801862801824297\n",
            "Epoch 5/10, Loss: 0.0002291427189009954, Val Loss: 2.734697405781065\n",
            "Epoch 6/10, Loss: 0.0001538270619305506, Val Loss: 2.7721207099301473\n",
            "Epoch 7/10, Loss: 0.00013384347254975215, Val Loss: 2.795907139778137\n",
            "Epoch 8/10, Loss: 0.00013520376254476746, Val Loss: 2.8266061586993083\n",
            "Epoch 9/10, Loss: 9.224263622955832e-05, Val Loss: 2.8551370927265713\n",
            "Epoch 10/10, Loss: 7.144262990213638e-05, Val Loss: 2.884343441043581\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Uso del Modelo para la detección de poses"
      ],
      "metadata": {
        "id": "R8RVFrEEAu5j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Se definen las herramientas a usar para el programa final\n",
        "from ultralytics import YOLO\n",
        "import numpy as np\n",
        "import torch\n",
        "import timm\n",
        "from torchvision import transforms\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import cv2\n",
        "\n",
        "# Carga del modelo\n",
        "model_pose = YOLO('yolov8n-pose.pt')\n",
        "model_path = '/content/drive/MyDrive/Base de Datos Duckietown/Modelo4/duckietown-pose-model.pth'\n",
        "num_classes = 4\n",
        "model = timm.create_model('timm/resnet50.a1_in1k', pretrained=False, num_classes=num_classes)\n",
        "model.load_state_dict(torch.load(model_path)) # Se agregan los pesos del entrenamiento\n",
        "model.eval() # Se deja el modelo en modo evaluación\n",
        "\n",
        "# Función para aislar en un fondo negro la pose de una imagen.\n",
        "def isolate_pose(path):\n",
        "  model = YOLO('yolov8n-pose.pt') # Modelo YOLO Pose para la detección\n",
        "  results = model(path)\n",
        "  original = results[0].orig_shape\n",
        "  h = int(original[0])\n",
        "  w = int(original[1])\n",
        "  black_img = np.zeros((h,w,3),dtype=np.uint8) # Se genera imagen negra con alto y largo de la imagen original\n",
        "  img_array = results[0].plot(img=black_img, labels=False, boxes=False, probs=False)\n",
        "  im = Image.fromarray(img_array[..., ::-1])\n",
        "  return im\n",
        "\n",
        "# Transformaciones a realizar en las imágenes\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Preprocesamiento imagen\n",
        "def preprocess_image(image):\n",
        "    #image = Image.open(image)\n",
        "    image = transform(image)\n",
        "    image = image.unsqueeze(0)\n",
        "    return image\n",
        "\n",
        "# Función para predecir con el modelo\n",
        "def model_predict(model, image):\n",
        "    with torch.no_grad():\n",
        "        outputs = model(image)\n",
        "    return outputs"
      ],
      "metadata": {
        "id": "pKADvOYbA7v_"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A continuación se tiene la función con la que se realiza el análisis de pose, sólo se debe correr el código de arriba para que pueda funcionar y usar la función dándole como única variable la ruta hacia la imagen/video que se quiere estudiar."
      ],
      "metadata": {
        "id": "Vh98xlYwKgh8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Función para detectar el ejercicio que se está realizando a partir de una imagen o video\n",
        "# La base de datos del entrenamiento actual contiene: persona de pie, sentadilla, flexión, peso muerto y press militar.\n",
        "\n",
        "def pose_detect(image_path):\n",
        "  classes = {\n",
        "    0: 'PesoMuerto',\n",
        "    1: 'Press',\n",
        "    2: 'Squat',\n",
        "    3: 'Standing'}\n",
        "  pose_img = isolate_pose(image_path)\n",
        "  pre_img = preprocess_image(pose_img)\n",
        "  results = model_predict(model, pre_img)\n",
        "  probabilidades, predicciones = torch.max(results, 1)\n",
        "  predicted_class = classes[predicciones.item()]\n",
        "  text = f\"Clase predicha: {predicted_class}, Probabilidad: {probabilidades.item()}\"\n",
        "  print(text)\n",
        "  draw = ImageDraw.Draw(pose_img) # Se escriben los resultados sobre la imagen con la pose\n",
        "  font = ImageFont.truetype('/content/drive/MyDrive/Base de Datos Duckietown/Roboto-Medium.ttf', size=20) # Se elige la fuente para escribir\n",
        "  position = (10, 10) # Se define la posición en la que estará el texto\n",
        "  draw.text(position, text, fill=(255, 255, 255), font=font)\n",
        "  return pose_img"
      ],
      "metadata": {
        "id": "vIPX6N8yIF0D"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Y por último, se tiene una función para la detección en tiempo real, esto se debe correr localmente en un dispositivo con GPU disponible y cámara funcional."
      ],
      "metadata": {
        "id": "o5bpw1Z1T9Q_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Detección en tiempo real con la función\n",
        "import cv2\n",
        "\n",
        "def pose_detect_camera():\n",
        "    classes = {\n",
        "        0: 'PesoMuerto',\n",
        "        1: 'Press',\n",
        "        2: 'Squat',\n",
        "        3: 'Standing'\n",
        "    }\n",
        "\n",
        "    # Iniciar la captura de video con la cámara\n",
        "    cap = cv2.VideoCapture(0) # 0 es para la cámara default\n",
        "\n",
        "    while True:\n",
        "        # Capturar frame de la cámara\n",
        "        ret, frame = cap.read()\n",
        "\n",
        "        # Si no se puede capturar el frame, salir del bucle\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Convertir el frame de BGR a RGB\n",
        "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Convertir el frame a una imagen de PIL\n",
        "        pil_img = Image.fromarray(rgb_frame)\n",
        "\n",
        "        # Aplicar el análisis de pose al frame\n",
        "        pose_img = pose_detect(pil_img)\n",
        "\n",
        "        # Mostrar el frame resultante con las detecciones\n",
        "        cv2.imshow('Pose Detection', np.array(pose_img)[:, :, ::-1])\n",
        "\n",
        "        # Salir del bucle si se presiona la tecla 'q'\n",
        "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "            break\n",
        "\n",
        "    # Liberar recursos\n",
        "    cap.release()\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "# Usar la función para iniciar la detección en tiempo real\n",
        "pose_detect_camera()"
      ],
      "metadata": {
        "id": "Z27m6_e4a4As"
      },
      "execution_count": 39,
      "outputs": []
    }
  ]
}